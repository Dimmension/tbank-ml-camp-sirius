{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["IsJpIuv31d-_","8dxw_cUx1d_A","0uozwoRw1d_B"]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10148109,"sourceType":"datasetVersion","datasetId":6264491},{"sourceId":10156229,"sourceType":"datasetVersion","datasetId":6270634},{"sourceId":10157699,"sourceType":"datasetVersion","datasetId":6271781},{"sourceId":10161451,"sourceType":"datasetVersion","datasetId":6274670}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence_transformers\n# !pip install -U FlagEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:13.675190Z","iopub.execute_input":"2024-12-11T12:31:13.675906Z","iopub.status.idle":"2024-12-11T12:31:23.628432Z","shell.execute_reply.started":"2024-12-11T12:31:13.675864Z","shell.execute_reply":"2024-12-11T12:31:23.627359Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\n# from FlagEmbedding import BGEM3FlagModel\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:23.634538Z","iopub.execute_input":"2024-12-11T12:31:23.634849Z","iopub.status.idle":"2024-12-11T12:31:24.084899Z","shell.execute_reply.started":"2024-12-11T12:31:23.634812Z","shell.execute_reply":"2024-12-11T12:31:24.083836Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path = '/kaggle/input/balanced/data_full_spoiled.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:24.087401Z","iopub.execute_input":"2024-12-11T12:31:24.087924Z","iopub.status.idle":"2024-12-11T12:31:24.092985Z","shell.execute_reply.started":"2024-12-11T12:31:24.087878Z","shell.execute_reply":"2024-12-11T12:31:24.091865Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = json.load(open(path))\n\ntrain = pd.DataFrame(df['train'], columns=['text','intent'])\nval = pd.DataFrame(df['val'],columns=['text','intent'])\noos_train = pd.DataFrame(df['oos_train'], columns=['text','intent'])\noos_val = pd.DataFrame(df['oos_val'],columns=['text','intent'])\n\ntrain['text_len'] = train['text'].map(len)\nval['text_len'] = val['text'].map(len)\noos_train['text_len'] = oos_train['text'].map(len)\noos_val['text_len'] = oos_val['text'].map(len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:24.094188Z","iopub.execute_input":"2024-12-11T12:31:24.094530Z","iopub.status.idle":"2024-12-11T12:31:24.151671Z","shell.execute_reply.started":"2024-12-11T12:31:24.094492Z","shell.execute_reply":"2024-12-11T12:31:24.150862Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"descriptions = json.load(open('/kaggle/input/new-descriptions2/labels_with_description.json'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:24.152658Z","iopub.execute_input":"2024-12-11T12:31:24.152914Z","iopub.status.idle":"2024-12-11T12:31:24.162803Z","shell.execute_reply.started":"2024-12-11T12:31:24.152889Z","shell.execute_reply":"2024-12-11T12:31:24.161837Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_df = pd.concat((train, oos_train)).drop(columns='text_len')\nval_df = pd.concat((val, oos_val)).drop(columns='text_len')\ntrain_df['description'] = train_df['intent'].map(lambda x: descriptions[x] if x != 'oos' else None)\nval_df['description'] = val_df['intent'].map(lambda x: descriptions[x] if x != 'oos' else None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:24.164075Z","iopub.execute_input":"2024-12-11T12:31:24.164310Z","iopub.status.idle":"2024-12-11T12:31:24.184826Z","shell.execute_reply.started":"2024-12-11T12:31:24.164286Z","shell.execute_reply":"2024-12-11T12:31:24.184063Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:34:05.331947Z","iopub.status.idle":"2024-12-11T11:34:05.332363Z","shell.execute_reply.started":"2024-12-11T11:34:05.332148Z","shell.execute_reply":"2024-12-11T11:34:05.332178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = BGEM3FlagModel('BAAI/bge-m3', use_bfp16=True) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:15:10.814230Z","iopub.execute_input":"2024-12-11T12:15:10.814766Z","iopub.status.idle":"2024-12-11T12:15:10.818444Z","shell.execute_reply.started":"2024-12-11T12:15:10.814732Z","shell.execute_reply":"2024-12-11T12:15:10.817454Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# n = len(descriptions)\n# pairs = []\n# nearest_labels = {}\n\n# for i in tqdm(range(n)):\n#     embeddings_1 = model.encode(descriptions[i])['dense_vecs']\n#     cur_nearest = []\n#     max_similarity = 0\n#     for j in range(n):\n#         if i != j:\n#             embeddings_2 = model.encode(descriptions[j])['dense_vecs']\n#             similarity = embeddings_1 @ embeddings_2.T\n#             if similarity >= max_similarity:\n#                 cur_nearest.append(unique_labels[j])\n#                 max_similarity = similarity\n#                 if len(cur_nearest) > 3:\n#                     cur_nearest.pop(0)\n                \n#     nearest_labels[unique_labels[i]] = cur_nearest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:15:15.777826Z","iopub.execute_input":"2024-12-11T12:15:15.778117Z","iopub.status.idle":"2024-12-11T12:15:15.782127Z","shell.execute_reply.started":"2024-12-11T12:15:15.778093Z","shell.execute_reply":"2024-12-11T12:15:15.781201Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# a = pd.read_csv('/kaggle/input/descriptions-with-context/label_desc_context_new.csv')\n# a['nearest_labels'] = a['intent'].map(lambda x: nearest_labels[x] if x != 'oos' else None)\n# a.to_csv('label_desc_context_nearest_many.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T11:36:14.498369Z","iopub.execute_input":"2024-12-11T11:36:14.498695Z","iopub.status.idle":"2024-12-11T11:36:14.515972Z","shell.execute_reply.started":"2024-12-11T11:36:14.498659Z","shell.execute_reply":"2024-12-11T11:36:14.515262Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"uniques = val_df.dropna().groupby('intent').agg('first').reset_index(drop=True)\ndescriptions = uniques['description'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:24.185959Z","iopub.execute_input":"2024-12-11T12:31:24.186287Z","iopub.status.idle":"2024-12-11T12:31:24.206535Z","shell.execute_reply.started":"2024-12-11T12:31:24.186252Z","shell.execute_reply":"2024-12-11T12:31:24.205783Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:24.207699Z","iopub.execute_input":"2024-12-11T12:31:24.208049Z","iopub.status.idle":"2024-12-11T12:31:28.420846Z","shell.execute_reply.started":"2024-12-11T12:31:24.207990Z","shell.execute_reply":"2024-12-11T12:31:28.419954Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = \"Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either 'Yes' or 'No'.\"\n    sep = \"\\n\"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)['input_ids']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)['input_ids']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f'A: {query}',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f'B: {passage}',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs['input_ids'],\n            sep_inputs + passage_inputs['input_ids'],\n            truncation='only_second',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item['input_ids'] = item['input_ids'] + sep_inputs + prompt_inputs\n        item['attention_mask'] = [1] * len(item['input_ids'])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors='pt',\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:31:28.422584Z","iopub.execute_input":"2024-12-11T12:31:28.423007Z","iopub.status.idle":"2024-12-11T12:31:28.430711Z","shell.execute_reply.started":"2024-12-11T12:31:28.422967Z","shell.execute_reply":"2024-12-11T12:31:28.429771Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-v2-gemma')\nmodel = AutoModelForCausalLM.from_pretrained('BAAI/bge-reranker-v2-gemma')\nyes_loc = tokenizer('Yes', add_special_tokens=False)['input_ids'][0]\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:37:20.058377Z","iopub.execute_input":"2024-12-11T12:37:20.059197Z","iopub.status.idle":"2024-12-11T12:37:23.113906Z","shell.execute_reply.started":"2024-12-11T12:37:20.059163Z","shell.execute_reply":"2024-12-11T12:37:23.113130Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c534c5018a4232aaae47206895c267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff553ef45b145d4ae67fb9aba1017d7"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:37:33.622388Z","iopub.execute_input":"2024-12-11T12:37:33.622735Z","iopub.status.idle":"2024-12-11T12:37:36.909632Z","shell.execute_reply.started":"2024-12-11T12:37:33.622704Z","shell.execute_reply":"2024-12-11T12:37:36.908912Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:38:30.482655Z","iopub.execute_input":"2024-12-11T12:38:30.482977Z","iopub.status.idle":"2024-12-11T12:38:30.788883Z","shell.execute_reply.started":"2024-12-11T12:38:30.482948Z","shell.execute_reply":"2024-12-11T12:38:30.787813Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for i in range(25):\n    query = uniques.iloc[i]['text']\n    pairs = [[query, description] for description in descriptions]\n    with torch.no_grad():\n        inputs = get_inputs(pairs[:25], tokenizer)\n        scores = model(**inputs.to(device), return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n        print(max(scores))\n\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T12:38:35.984917Z","iopub.execute_input":"2024-12-11T12:38:35.985587Z","iopub.status.idle":"2024-12-11T12:39:22.759120Z","shell.execute_reply.started":"2024-12-11T12:38:35.985554Z","shell.execute_reply":"2024-12-11T12:39:22.757952Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"tensor(1.8161, device='cuda:0')\ntensor(3.9061, device='cuda:0')\ntensor(3.9254, device='cuda:0')\ntensor(6.8830, device='cuda:0')\ntensor(8.7083, device='cuda:0')\ntensor(-1.8232, device='cuda:0')\ntensor(-0.6718, device='cuda:0')\ntensor(-4.9767, device='cuda:0')\ntensor(-1.6897, device='cuda:0')\ntensor(3.7225, device='cuda:0')\ntensor(2.4931, device='cuda:0')\ntensor(2.9321, device='cuda:0')\ntensor(-0.5184, device='cuda:0')\ntensor(2.1933, device='cuda:0')\ntensor(-0.7327, device='cuda:0')\ntensor(4.5785, device='cuda:0')\ntensor(3.2606, device='cuda:0')\ntensor(3.0053, device='cuda:0')\ntensor(7.1529, device='cuda:0')\ntensor(2.2075, device='cuda:0')\ntensor(0.2899, device='cuda:0')\ntensor(-1.2381, device='cuda:0')\ntensor(2.9346, device='cuda:0')\ntensor(5.4343, device='cuda:0')\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m get_inputs(pairs[:\u001b[38;5;241m25\u001b[39m], tokenizer)\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs\u001b[38;5;241m.\u001b[39mto(device), return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, yes_loc]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, )\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}